# Internship_Task_3
Objective: Implement and understand simple &amp; multiple linear regression.
# Internship_Task_3
Implement and understand simple & multiple linear regression.

# ğŸ  Internship Task 3: Linear Regression Analysis â€“ Housing Dataset

## ğŸ¯ Objective
The objective of this task is to implement and understand **Simple Linear Regression** and **Multiple Linear Regression** models to predict housing prices. We aim to compare model performance, interpret coefficients, and extract business insights from real estate data.

## ğŸ§  What You'll Learn
- Linear regression mathematical concepts and assumptions
- Simple vs Multiple regression implementation
- Model evaluation metrics (RÂ², RMSE, MAE, MSE)
- Feature importance and coefficient interpretation
- Residual analysis and model diagnostics
- Business insights extraction from regression models

## ğŸ“¦ Dataset Info
- **Name**: Housing.csv
- **Rows**: 545
- **Columns**: 13
- **Target variable**: price
- **Features**: area, bedrooms, bathrooms, stories, parking, mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea, furnishingstatus

## ğŸ“Œ Linear Regression Analysis Summary â€“ Interpretation

### ğŸ¯ Objective of Linear Regression
The main goal of Linear Regression Analysis is to model the relationship between features and target variable. This helps in:
- Predicting house prices based on property characteristics
- Understanding which features drive price variations
- Quantifying the impact of each feature on property value
- Making data-driven real estate investment decisions
- Building interpretable predictive models

### ğŸ” Process & Approach

#### 1. Data Preparation
- Loaded Housing.csv dataset using `pandas.read_csv()`
- Checked data quality: **545 rows, 13 columns, 0 missing values**
- Encoded categorical variables: yes/no â†’ 1/0, furnishing status â†’ 0/1/2

#### 2. Exploratory Data Analysis
- **Price Range**: â‚¹1,750,000 - â‚¹13,300,000 (Mean: â‚¹4,766,729)
- **Area Range**: 1,650 - 16,200 sq ft (Mean: 5,150 sq ft)
- Created correlation matrix to identify feature relationships
- Visualized distributions and relationships through scatter plots

#### 3. Simple Linear Regression
- **Selected Feature**: area (highest correlation: 0.5360)
- **Model**: Price = 2,512,254.26 + 425.73 Ã— Area
- **Train-Test Split**: 80-20 (436 training, 109 testing samples)

#### 4. Multiple Linear Regression
- **Features Used**: All 12 features after encoding
- **Model**: Comprehensive equation with 12 coefficients
- **Same train-test split** for fair comparison

#### 5. Model Evaluation & Comparison
- Calculated RÂ², RMSE, MAE, MSE for both models
- Created residual plots and diagnostic visualizations
- Performed coefficient interpretation and business analysis

### ğŸ“ˆ Tools & Libraries Used
- **Pandas** â€“ data manipulation and preprocessing
- **Scikit-learn** â€“ model implementation and evaluation
- **Matplotlib** â€“ static visualizations
- **Seaborn** â€“ advanced statistical plots
- **NumPy** â€“ numerical computations
- **Scipy** â€“ statistical analysis and Q-Q plots

## âœ… Key Results & Insights

### Model Performance Comparison
| Metric | Simple Linear Regression | Multiple Linear Regression | Improvement |
|--------|-------------------------|---------------------------|-------------|
| **RÂ² Score** | 27.29% | 64.95% | **+137.99%** |
| **RMSE** | â‚¹1,917,104 | â‚¹1,331,071 | **-30.57%** |
| **MAE** | â‚¹1,474,748 | â‚¹979,680 | **-33.57%** |

### Feature Impact Analysis (Multiple Regression)
| Feature | Coefficient | Business Impact |
|---------|-------------|-----------------|
| **Bathrooms** | +â‚¹1,097,117 | Each bathroom adds ~â‚¹1.1M to house value |
| **Air Conditioning** | +â‚¹785,551 | AC increases property value by â‚¹785K |
| **Hot Water Heating** | +â‚¹687,881 | Heating system adds â‚¹688K premium |
| **Preferred Area** | +â‚¹629,902 | Prime location commands â‚¹630K premium |
| **Stories** | +â‚¹406,223 | Each additional floor adds â‚¹406K |

### Business Insights
- **Bathrooms** have the highest impact on price (â‚¹1.1M per bathroom)
- **Area** remains fundamental but other amenities significantly influence pricing
- **Location and modern amenities** command substantial premiums
- **Multiple regression** explains 2.4x more variance than simple regression

## ğŸ’¬ Interview Q&A (Conceptual Understanding)

### 1. What assumptions does linear regression make?
Linear regression makes several key assumptions:
- **Linearity**: Relationship between features and target is linear
- **Independence**: Observations are independent of each other
- **Homoscedasticity**: Constant variance of residuals (equal spread)
- **Normality**: Residuals are normally distributed
- **No Multicollinearity**: Features are not highly correlated with each other

*In our housing analysis*: We validated these through residual plots, Q-Q plots, and VIF analysis (all values < 2.0).

### 2. How do you interpret the coefficients?
Each coefficient represents the **change in target variable for a unit change in that feature**, holding all other variables constant. 

*In our housing model*:
- **Bathrooms coefficient (+â‚¹1,097,117)**: Adding one bathroom increases house price by â‚¹1.1M
- **Area coefficient (+â‚¹235.85)**: Each additional sq ft increases price by â‚¹236
- **AC coefficient (+â‚¹785,551)**: Having AC increases property value by â‚¹785K

### 3. What is RÂ² score and its significance?
**RÂ² (R-squared)** measures the proportion of variance in the target variable explained by the model. Range: 0 to 1 (0% to 100%).

*Our results*:
- **Simple Regression RÂ² = 27.29%**: Area alone explains 27% of price variation
- **Multiple Regression RÂ² = 64.95%**: All features together explain 65% of price variation
- **Significance**: Higher RÂ² indicates better model fit and predictive power

### 4. When would you prefer MSE over MAE?
**MSE (Mean Squared Error)** vs **MAE (Mean Absolute Error)**:
- **Use MSE when**: Large errors are disproportionately bad (penalizes outliers heavily)
- **Use MAE when**: All errors are equally important (robust to outliers)

*In our housing analysis*:
- **MSE = 1.77Ã—10Â¹Â²**: Heavily penalizes large price prediction errors
- **MAE = â‚¹979,680**: Average absolute error, more interpretable for business
- **Preference**: MSE for model training (gradient-friendly), MAE for business interpretation

### 5. How do you detect multicollinearity?
**Detection methods**:
- **Correlation Matrix**: Check pairwise correlations (>0.8 indicates high correlation)
- **VIF (Variance Inflation Factor)**: VIF > 5-10 indicates multicollinearity
- **Condition Number**: High values (>30) suggest multicollinearity

*Our analysis results*:
- **VIF values**: All features < 2.0 (excellent - no multicollinearity)
- **Correlation matrix**: Highest correlation between features was moderate
- **Conclusion**: Coefficients are stable and reliable

### 6. What is the difference between simple and multiple regression?
| Aspect | Simple Linear Regression | Multiple Linear Regression |
|--------|-------------------------|---------------------------|
| **Variables** | One predictor (x) | Multiple predictors (xâ‚, xâ‚‚, ..., xâ‚™) |
| **Equation** | y = Î²â‚€ + Î²â‚x + Îµ | y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ |
| **RÂ² (Our Results)** | 27.29% | 64.95% |
| **Interpretation** | Direct relationship | Considers multiple factors simultaneously |
| **Use Case** | Simple relationships | Complex real-world scenarios |

### 7. Can linear regression be used for classification?
**Technically possible but not recommended**:
- **Issues**: Outputs continuous values, not probabilities
- **Problems**: Can predict values outside 0-1 range
- **Better alternatives**: Logistic Regression, SVM, Decision Trees

**However**: Linear regression can be adapted:
- **Threshold approach**: Use 0.5 cutoff for binary classification
- **Feature engineering**: Create binary outcomes from continuous predictions
- **Our housing context**: Could classify "Expensive" vs "Affordable" housing

### 8. What happens if you violate regression assumptions?
**Consequences of assumption violations**:

| Assumption Violated | Consequences | Solutions |
|-------------------|-------------|-----------|
| **Linearity** | Biased predictions, poor fit | Transform variables, polynomial features |
| **Independence** | Incorrect standard errors | Time series analysis, clustered errors |
| **Homoscedasticity** | Unreliable confidence intervals | Weighted regression, log transformation |
| **Normality** | Invalid hypothesis tests | Large samples (CLT), non-parametric methods |
| **No Multicollinearity** | Unstable coefficients | Remove correlated features, PCA, Ridge regression |

*In our analysis*: We checked all assumptions and found reasonable compliance, making our model reliable for housing price prediction.

## ğŸ“Š Model Diagnostics

### Residual Analysis Results
- **Residuals distribution**: Approximately normal with slight right skew
- **Homoscedasticity**: Reasonable constant variance assumption met
- **No obvious patterns**: In residual plots, indicating good model fit

### Statistical Validation
- **VIF Analysis**: All values < 2.0 (low multicollinearity)
- **Coefficient Significance**: All major features show meaningful impact
- **Model Stability**: Consistent performance across train-test splits

## ğŸš€ Future Enhancements
- **Polynomial Features**: Capture non-linear relationships
- **Interaction Terms**: Model feature combinations (area Ã— amenities)
- **Regularization**: Ridge/Lasso regression for feature selection
- **External Data**: Add neighborhood, economic, and temporal factors

## ğŸ“š Final Note
Linear regression provides an excellent foundation for understanding **predictive modeling** and **feature relationships**. This analysis demonstrates how multiple features significantly improve prediction accuracy and provide actionable business insights for real estate decision-making.

**Key Takeaway**: While area is the strongest individual predictor, the combination of location, amenities, and structural features creates a comprehensive model that explains nearly 65% of housing price variation.
